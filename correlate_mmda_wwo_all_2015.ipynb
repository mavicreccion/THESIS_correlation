{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import minmax_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get weather dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_dataset = pd.read_csv('weather_wwo_manila_2015.csv', skipinitialspace=True, encoding='cp1252')\n",
    "cols = [\"tempC\", \"windspeedKmph\", \"cond\", \"precipMM\", \"humidity\", \"visibility\", \"pressure\", \"cloudcover\", \"heatIndexC\", \"dewPointC\", \"windChillC\", \"windGustKmph\", \"feelsLikeC\"]\n",
    "date_dataset = weather_dataset[\"dt\"]\n",
    "weather_dataset = weather_dataset[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlate traffic and weather dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roads = [\"A. Maceda\", \"Anda Circle\", \"Antipolo\", \"Bluementritt\", \"Buendia\", \"Edsa Extension\", \"Finance Road\", \"Gov. Forbes - Lacson\", \"Lerma\", \"Magsaysay Ave\", \"P.Noval\", \"Pablo Ocampo\", \"Pedro Gil\", \"Quezon Ave.\", \"Quirino\", \"Rajah Sulayman\", \"Taft Ave.\", \"U.N. Avenue\", \"Vicente Cruz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for road in roads:\n",
    "    traffic_dataset = pd.read_csv('mmda_2015_transformed/mmda_' + road + '_2015_transformed.csv', skipinitialspace=True, encoding='cp1252')\n",
    "    traffic_dataset = traffic_dataset[['statusN', 'statusS']]\n",
    "    \n",
    "    # merge traffic and weather dataset\n",
    "    dataset = pd.concat([traffic_dataset, weather_dataset], axis=1, join='inner')\n",
    "    \n",
    "    # correlate\n",
    "    corr = dataset.corr(method='spearman')\n",
    "    corr.to_csv('corr_mmda_wwo_2015/corr_results_mmda_wwo_' + road + '_2015.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlate traffic and weather dataset wth lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roads = [\"A. Maceda\", \"Anda Circle\", \"Antipolo\", \"Bluementritt\", \"Buendia\", \"Edsa Extension\", \"Finance Road\", \"Gov. Forbes - Lacson\", \"Lerma\", \"Magsaysay Ave\", \"P.Noval\", \"Pablo Ocampo\", \"Pedro Gil\", \"Quezon Ave.\", \"Quirino\", \"Rajah Sulayman\", \"Taft Ave.\", \"U.N. Avenue\", \"Vicente Cruz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_lags = 8\n",
    "\n",
    "for road in roads:\n",
    "    traffic_dataset = pd.read_csv('mmda_2015_transformed/mmda_' + road + '_2015_transformed.csv', skipinitialspace=True, encoding='cp1252')\n",
    "    traffic_dataset = traffic_dataset[['statusN', 'statusS']]\n",
    "    \n",
    "    # merge traffic and weather dataset\n",
    "    dataset = pd.concat([traffic_dataset, weather_dataset], axis=1, join='inner')\n",
    "    \n",
    "    # make a copy\n",
    "    new_dataset = dataset.copy()\n",
    "    \n",
    "    for i in range(1, (total_lags+1)):\n",
    "        new_dataset.statusN = new_dataset.statusN.shift(-1)\n",
    "        new_dataset.statusS = new_dataset.statusS.shift(-1)\n",
    "        new_dataset = new_dataset[:(len(new_dataset)-1)]\n",
    "\n",
    "        corr = new_dataset.corr(method='spearman')\n",
    "        corr.to_csv('corr_mmda_wwo_2015_lags/corr_mmda_wwo_' + road + '_2015_lag_' + str(i) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_lags' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-5a2fab317ebf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtotal_lags\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mlag_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'total_lags' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1, (total_lags+1)):\n",
    "    \n",
    "    lag_dataset = []\n",
    "    \n",
    "    for road in roads:\n",
    "        dataset = pd.read_csv('corr_mmda_wwo_2015_lags/corr_mmda_wwo_' + road + '_2015_lag_' + str(i) + '.csv', skipinitialspace=True, encoding='cp1252')\n",
    "        dataset = dataset.loc[:, ~dataset.columns.str.contains('^Unnamed')]\n",
    "        dataset = dataset.loc[:1]\n",
    "        \n",
    "        lag_dataset.append(dataset)\n",
    "    \n",
    "    lag_dataset = pd.concat(lag_dataset)\n",
    "    lag_dataset.to_csv('corr_mmda_wwo_2015_lags/corr_mmda_wwo_2015_lag_' + str(i) + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlate traffic and weather with wet dry seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getWetDrySeasons(my_dataset):\n",
    "    # choose wet (jun to oct) season\n",
    "    start_date = '2015-06-01 00:00:00'\n",
    "    end_date = '2015-10-31 23:45:00'\n",
    "    wet_mask = (my_dataset['dt'] >= start_date) & (my_dataset['dt'] <= end_date)\n",
    "    wet_dataset = my_dataset.loc[wet_mask]\n",
    "    \n",
    "    # choose dry (nov to dec, jan to may)\n",
    "    first_start_date = '2015-01-01 00:00:00'\n",
    "    first_end_date = '2015-05-31 23:45:00'\n",
    "    second_start_date = '2015-11-1 00:00:00'\n",
    "    second_end_date = '2015-12-31 23:45:00'\n",
    "    first_dry_mask = (my_dataset['dt'] >= first_start_date) & (my_dataset['dt'] <= first_end_date)\n",
    "    second_dry_mask = (my_dataset['dt'] >= second_start_date) & (my_dataset['dt'] <= second_end_date)\n",
    "    dry_dataset = my_dataset.loc[first_dry_mask]\n",
    "    dry_dataset = pd.concat([dry_dataset, my_dataset.loc[second_dry_mask]])\n",
    "    \n",
    "    return wet_dataset, dry_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for road in roads:\n",
    "    traffic_dataset = pd.read_csv('mmda_2015_transformed/mmda_' + road + '_2015_transformed.csv', skipinitialspace=True, encoding='cp1252')\n",
    "    traffic_dataset = traffic_dataset[['dt', 'statusN', 'statusS']]\n",
    "    \n",
    "    # merge traffic and weather dataset\n",
    "    dataset = pd.concat([traffic_dataset, weather_dataset], axis=1, join='inner')\n",
    "    \n",
    "    # ensure dt is datetime\n",
    "    dataset['dt'] = pd.to_datetime(dataset['dt']) \n",
    "    \n",
    "    # get wet and dry season dataset\n",
    "    wet_dataset, dry_dataset = getWetDrySeasons(dataset)\n",
    "    \n",
    "    # correlate\n",
    "    corr = wet_dataset.corr(method='spearman')\n",
    "    corr.to_csv('corr_mmda_wwo_2015_seasons_wetdry/wet_corr_mmda_wwo_' + road + '_2015.csv')\n",
    "    corr = dry_dataset.corr(method='spearman')\n",
    "    corr.to_csv('corr_mmda_wwo_2015_seasons_wetdry/dry_corr_mmda_wwo_' + road + '_2015.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wet_df = []\n",
    "dry_df = []\n",
    "\n",
    "for road in roads:\n",
    "    wet_dataset = pd.read_csv('corr_mmda_wwo_2015_seasons_wetdry/wet_corr_mmda_wwo_' + road + '_2015.csv', skipinitialspace=True, encoding='cp1252')\n",
    "    wet_dataset = wet_dataset.loc[:, ~wet_dataset.columns.str.contains('^Unnamed')]\n",
    "    wet_dataset = wet_dataset.loc[:1]\n",
    "    wet_df.append(wet_dataset)\n",
    "    \n",
    "    dry_dataset = pd.read_csv('corr_mmda_wwo_2015_seasons_wetdry/dry_corr_mmda_wwo_' + road + '_2015.csv', skipinitialspace=True, encoding='cp1252')\n",
    "    dry_dataset = dry_dataset.loc[:, ~dry_dataset.columns.str.contains('^Unnamed')]\n",
    "    dry_dataset = dry_dataset.loc[:1]\n",
    "    dry_df.append(dry_dataset)\n",
    "    \n",
    "wet_df = pd.concat(wet_df)\n",
    "wet_df.to_csv('corr_mmda_wwo_2015_seasons_wetdry/wet_corr_mmda_wwo_2015.csv')\n",
    "dry_df = pd.concat(dry_df)\n",
    "dry_df.to_csv('corr_mmda_wwo_2015_seasons_wetdry/dry_corr_mmda_wwo_2015.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlate traffic and weather dataset with wet dry seasons and lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_lags = 8\n",
    "\n",
    "for road in roads:\n",
    "    traffic_dataset = pd.read_csv('mmda_2015_transformed/mmda_' + road + '_2015_transformed.csv', skipinitialspace=True, encoding='cp1252')\n",
    "    traffic_dataset = traffic_dataset[['dt', 'statusN', 'statusS']]\n",
    "    \n",
    "    # merge traffic and weather dataset\n",
    "    dataset = pd.concat([traffic_dataset, weather_dataset], axis=1, join='inner')\n",
    "    \n",
    "    # get wet and dry season dataset\n",
    "    wet_dataset, dry_dataset = getWetDrySeasons(dataset)\n",
    "    \n",
    "    # make a copy\n",
    "    new_wet_dataset = wet_dataset.copy()\n",
    "    new_dry_dataset = dry_dataset.copy()\n",
    "    \n",
    "    for i in range(1, (total_lags+1)):\n",
    "        new_wet_dataset.statusN = new_wet_dataset.statusN.shift(-1)\n",
    "        new_wet_dataset.statusS = new_wet_dataset.statusS.shift(-1)\n",
    "        new_wet_dataset = new_wet_dataset[:(len(new_wet_dataset)-1)]\n",
    "\n",
    "        corr = new_wet_dataset.corr(method='spearman')\n",
    "        corr.to_csv('corr_mmda_wwo_2015_seasons_wetdry_lags/wet_corr_mmda_wwo_' + road + '_2015_lag_' + str(i) + '.csv')\n",
    "        \n",
    "        new_dry_dataset.statusN = new_dry_dataset.statusN.shift(-1)\n",
    "        new_dry_dataset.statusS = new_dry_dataset.statusS.shift(-1)\n",
    "        new_dry_dataset = new_dry_dataset[:(len(new_dry_dataset)-1)]\n",
    "\n",
    "        corr = new_dry_dataset.corr(method='spearman')\n",
    "        corr.to_csv('corr_mmda_wwo_2015_seasons_wetdry_lags/dry_corr_mmda_wwo_' + road + '_2015_lag_' + str(i) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1, (total_lags+1)):\n",
    "    \n",
    "    wet_lag_dataset = []\n",
    "    dry_lag_dataset = []\n",
    "    \n",
    "    for road in roads:\n",
    "        wet_dataset = pd.read_csv('corr_mmda_wwo_2015_seasons_wetdry_lags/wet_corr_mmda_wwo_' + road + '_2015_lag_' + str(i) + '.csv', skipinitialspace=True, encoding='cp1252')\n",
    "        wet_dataset = wet_dataset.loc[:, ~wet_dataset.columns.str.contains('^Unnamed')]\n",
    "        wet_dataset = wet_dataset.loc[:1]\n",
    "        wet_lag_dataset.append(wet_dataset)\n",
    "        \n",
    "        dry_dataset = pd.read_csv('corr_mmda_wwo_2015_seasons_wetdry_lags/dry_corr_mmda_wwo_' + road + '_2015_lag_' + str(i) + '.csv', skipinitialspace=True, encoding='cp1252')\n",
    "        dry_dataset = dry_dataset.loc[:, ~dry_dataset.columns.str.contains('^Unnamed')]\n",
    "        dry_dataset = dry_dataset.loc[:1]\n",
    "        dry_lag_dataset.append(dry_dataset)\n",
    "    \n",
    "    wet_lag_dataset = pd.concat(wet_lag_dataset)\n",
    "    wet_lag_dataset.to_csv('corr_mmda_wwo_2015_seasons_wetdry_lags/wet_corr_mmda_wwo_2015_lag_' + str(i) + '.csv')\n",
    "    dry_lag_dataset = pd.concat(dry_lag_dataset)\n",
    "    dry_lag_dataset.to_csv('corr_mmda_wwo_2015_seasons_wetdry_lags/dry_corr_mmda_wwo_2015_lag_' + str(i) + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlate traffic and weather dataset with cool hot seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roads = [\"A. Maceda\", \"Anda Circle\", \"Antipolo\", \"Bluementritt\", \"Buendia\", \"Edsa Extension\", \"Finance Road\", \"Gov. Forbes - Lacson\", \"Lerma\", \"Magsaysay Ave\", \"P.Noval\", \"Pablo Ocampo\", \"Pedro Gil\", \"Quezon Ave.\", \"Quirino\", \"Rajah Sulayman\", \"Taft Ave.\", \"U.N. Avenue\", \"Vicente Cruz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getCoolHotSeasons(my_dataset):\n",
    "    # choose hot (march to oct) season\n",
    "    start_date = '2015-03-01 00:00:00'\n",
    "    end_date = '2015-10-31 23:45:00'\n",
    "    hot_mask = (my_dataset['dt'] >= start_date) & (my_dataset['dt'] <= end_date)\n",
    "    hot_dataset = my_dataset.loc[hot_mask]\n",
    "    \n",
    "    # choose cool (nov to dec, jan to feb)\n",
    "    first_start_date = '2015-01-01 00:00:00'\n",
    "    first_end_date = '2015-02-28 23:45:00'\n",
    "    second_start_date = '2015-11-1 00:00:00'\n",
    "    second_end_date = '2015-12-31 23:45:00'\n",
    "    first_cool_mask = (my_dataset['dt'] >= first_start_date) & (my_dataset['dt'] <= first_end_date)\n",
    "    second_cool_mask = (my_dataset['dt'] >= second_start_date) & (my_dataset['dt'] <= second_end_date)\n",
    "    cool_dataset = my_dataset.loc[first_cool_mask]\n",
    "    cool_dataset = pd.concat([cool_dataset, my_dataset.loc[second_cool_mask]])\n",
    "    \n",
    "    return hot_dataset, cool_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for road in roads:\n",
    "    traffic_dataset = pd.read_csv('mmda_2015_transformed/mmda_' + road + '_2015_transformed.csv', skipinitialspace=True, encoding='cp1252')\n",
    "    traffic_dataset = traffic_dataset[['dt', 'statusN', 'statusS']]\n",
    "    \n",
    "    # merge traffic and weather dataset\n",
    "    dataset = pd.concat([traffic_dataset, weather_dataset], axis=1, join='inner')\n",
    "    \n",
    "    # ensure dt is datetime\n",
    "    dataset['dt'] = pd.to_datetime(dataset['dt']) \n",
    "    \n",
    "    # get hot and cool season dataset\n",
    "    hot_dataset, cool_dataset = getCoolHotSeasons(dataset)\n",
    "    \n",
    "    # correlate\n",
    "    corr = hot_dataset.corr(method='spearman')\n",
    "    corr.to_csv('corr_mmda_wwo_2015_seasons_hotcool/hot_corr_mmda_wwo_' + road + '_2015.csv')\n",
    "    corr = cool_dataset.corr(method='spearman')\n",
    "    corr.to_csv('corr_mmda_wwo_2015_seasons_hotcool/cool_corr_mmda_wwo_' + road + '_2015.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hot_df = []\n",
    "cool_df = []\n",
    "\n",
    "for road in roads:\n",
    "    hot_dataset = pd.read_csv('corr_mmda_wwo_2015_seasons_hotcool/hot_corr_mmda_wwo_' + road + '_2015.csv', skipinitialspace=True, encoding='cp1252')\n",
    "    hot_dataset = hot_dataset.loc[:, ~hot_dataset.columns.str.contains('^Unnamed')]\n",
    "    hot_dataset = hot_dataset.loc[:1]\n",
    "    hot_df.append(hot_dataset)\n",
    "    \n",
    "    cool_dataset = pd.read_csv('corr_mmda_wwo_2015_seasons_hotcool/cool_corr_mmda_wwo_' + road + '_2015.csv', skipinitialspace=True, encoding='cp1252')\n",
    "    cool_dataset = cool_dataset.loc[:, ~cool_dataset.columns.str.contains('^Unnamed')]\n",
    "    cool_dataset = cool_dataset.loc[:1]\n",
    "    cool_df.append(cool_dataset)\n",
    "    \n",
    "hot_df = pd.concat(hot_df)\n",
    "hot_df.to_csv('corr_mmda_wwo_2015_seasons_hotcool/hot_corr_mmda_wwo_2015.csv')\n",
    "cool_df = pd.concat(cool_df)\n",
    "cool_df.to_csv('corr_mmda_wwo_2015_seasons_hotcool/cool_corr_mmda_wwo_2015.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_lags = 8\n",
    "\n",
    "for road in roads:\n",
    "    traffic_dataset = pd.read_csv('mmda_2015_transformed/mmda_' + road + '_2015_transformed.csv', skipinitialspace=True, encoding='cp1252')\n",
    "    traffic_dataset = traffic_dataset[['dt', 'statusN', 'statusS']]\n",
    "    \n",
    "    # merge traffic and weather dataset\n",
    "    dataset = pd.concat([traffic_dataset, weather_dataset], axis=1, join='inner')\n",
    "    \n",
    "    # get hot and cool season dataset\n",
    "    hot_dataset, cool_dataset = getCoolHotSeasons(dataset)\n",
    "    \n",
    "    # make a copy\n",
    "    new_hot_dataset = hot_dataset.copy()\n",
    "    new_cool_dataset = cool_dataset.copy()\n",
    "    \n",
    "    for i in range(1, (total_lags+1)):\n",
    "        new_hot_dataset.statusN = new_hot_dataset.statusN.shift(-1)\n",
    "        new_hot_dataset.statusS = new_hot_dataset.statusS.shift(-1)\n",
    "        new_hot_dataset = new_hot_dataset[:(len(new_hot_dataset)-1)]\n",
    "\n",
    "        corr = new_hot_dataset.corr(method='spearman')\n",
    "        corr.to_csv('corr_mmda_wwo_2015_seasons_hotcool_lags/hot_corr_mmda_wwo_' + road + '_2015_lag_' + str(i) + '.csv')\n",
    "        \n",
    "        new_cool_dataset.statusN = new_cool_dataset.statusN.shift(-1)\n",
    "        new_cool_dataset.statusS = new_cool_dataset.statusS.shift(-1)\n",
    "        new_cool_dataset = new_cool_dataset[:(len(new_cool_dataset)-1)]\n",
    "\n",
    "        corr = new_cool_dataset.corr(method='spearman')\n",
    "        corr.to_csv('corr_mmda_wwo_2015_seasons_hotcool_lags/cool_corr_mmda_wwo_' + road + '_2015_lag_' + str(i) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1, (total_lags+1)):\n",
    "    \n",
    "    hot_lag_dataset = []\n",
    "    cool_lag_dataset = []\n",
    "    \n",
    "    for road in roads:\n",
    "        hot_dataset = pd.read_csv('corr_mmda_wwo_2015_seasons_hotcool_lags/hot_corr_mmda_wwo_' + road + '_2015_lag_' + str(i) + '.csv', skipinitialspace=True, encoding='cp1252')\n",
    "        hot_dataset = hot_dataset.loc[:, ~hot_dataset.columns.str.contains('^Unnamed')]\n",
    "        hot_dataset = hot_dataset.loc[:1]\n",
    "        hot_lag_dataset.append(hot_dataset)\n",
    "        \n",
    "        cool_dataset = pd.read_csv('corr_mmda_wwo_2015_seasons_hotcool_lags/cool_corr_mmda_wwo_' + road + '_2015_lag_' + str(i) + '.csv', skipinitialspace=True, encoding='cp1252')\n",
    "        cool_dataset = cool_dataset.loc[:, ~cool_dataset.columns.str.contains('^Unnamed')]\n",
    "        cool_dataset = cool_dataset.loc[:1]\n",
    "        cool_lag_dataset.append(cool_dataset)\n",
    "    \n",
    "    hot_lag_dataset = pd.concat(hot_lag_dataset)\n",
    "    hot_lag_dataset.to_csv('corr_mmda_wwo_2015_seasons_hotcool_lags/hot_corr_mmda_wwo_2015_lag_' + str(i) + '.csv')\n",
    "    cool_lag_dataset = pd.concat(cool_lag_dataset)\n",
    "    cool_lag_dataset.to_csv('corr_mmda_wwo_2015_seasons_hotcool_lags/cool_corr_mmda_wwo_2015_lag_' + str(i) + '.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
